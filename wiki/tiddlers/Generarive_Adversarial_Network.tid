created: 20160909061350995
modified: 20160921065203091
tags: [[Case study of ConvNets]] [[ICML 2015]] Blog Talks
title: Generarive Adversarial Network
type: text/vnd.tiddlywiki

GANs are recent approaches to modelling data in high dimensional spaces. Following are notes from [[Ian Goodfellow's ICML DL Workshop 2015 talk|https://www.youtube.com/watch?v=LXDuuYSNtUY]].

Papers relative in this post:

* Generative Adversarial Networks
* Conditional Generative Adversarial Nets
* On Distinguiashability Criteria for Estimating Generative Models
* Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks

! Generative modeling
In real life we always encounter problems where data generator are so complicated that its distribution cannot be evaluated. Probably drawn from super high dimensional space (image/audio). Suppose we have trainig examples $x\sim p_{data}(x)$ and want a model that can draw samples: $x\sim p_{model}(x)$ where $p_{model} = p_{data}$. Of course this is the idealized scenario. 

When given input of certain conditions, it can generate output with structure as rich as the input. This technique can be applied to speech synthesis, text translation etc. And it can generate data for other models, simulate the experiment environment and support reinforcement learning or simulate the reward of each action and use it in planning. 

Finally this might help us to leverage the unlabeled data problem. This possibility has not been demonstrated yet but we can assume the intermediate layers of GAN can be used as features for supervised tasks.

! Previous training methods
People always train a generative model that represents the probability distribution. And fit this model using maximum likelihood. 

$$
\theta^* = \underset{\theta}{max}\frac 1 m\sum_{i=1}^m\log p(x^{(i)};\theta)
$$

Until a few years ago the most popular way is to use undirected graphical models. And the flagship under the context of neural network is ''Deep Boltzmann machines'', where we define an energy function over the visible units and several hidden layers. We define an unnormalized probability distribution by taking $e$ to the negative energy fucntion, which has no constraint on it. And we normalize this positive functions to obtain a probability distribution function:

$$
\begin{align}
p(h, x) &= \frac 1 Z \tilde p(h, x)\\
\tilde p(h, x) &= \exp(-E(h, x))\\
Z&=\sum_{h, x}\tilde p(h, x)
\end{align}
$$

Unfortunately, computing that normalizing constant is NP-complete and our ability to approximate it is limited. In particular, if we train a Boltzmann machine well enough to represent the sharp distribution of widely separated modes, then our ability to approximate it decays. Our best approximation to the partition function is based on drawing samples from the model using ''MCMC''. And this Monte Carlo method works well as long as it is able to move between different modes. As the modes become sharper and better separated, we have a more difficult time transitioning between them. There are a lot of strategies to increase the temperature of the distribution temperarily in order to force the Markov chain to mix. But this problem is not solved to our satisfaction.

Some have been studying directed graphical models, where a similar problem arises. To compute the partition function, we need to sum over all configuartions of hidden states. ''Variational learning'' has been used to solve this. Where we maximize $\log p(x)-\mathcal D_{KL}(q(x)||p(z|x))$. And one of the most popular deep directed model is the ''Variational Autoencoder''. The basic idea is we have a distribution of the hidden units $q(z)$, and we can draw samples from this distribution with a neural network and some noise. This idea of a differentiable decoder, a.k.a. a generative net is very wide spread and serveral approaches to train the generative decoder nets.

Another popular model is generative stochastic networks. These networks learn the transition probability of the Markov chains. Which are very similar to GANs. The basic strategy is to give up on having an explicit formula for $p(x)$, just learn to sample incrementally. The drawback is it is difficult for Markov chains to mix from one example to another extremely dissimilar example.

! Generative adversarial networks
The general idea of GAN is to learn a sampling mechanism. We will learn to generate some input noise from a fixed prior distribution and then transform that noise into data space. 

* The advantage over generative stochastic network is there is no Markov chain involved so no issue of mixing.
* The advantage over VAE is that there is no variational lower bound.

!! Game theory: the basics
GANs are trained by //playing a game//. In game theory we consider the following senario:

	* N>1 players
	* Clearly defined set of actions each player can take
	* Clearly defined relationship between actions and outcomes
	* Clearly defined value of each outcome
	* Can't control the other player's actions, which is different from maximization.

Each player tries to maximize his reward during the game. While simulating the game, we are looking for an equilibrium form, where no player can improve his reward by changing their own strategies.

We are going to formulate the simplest GAN as a two-player zero-sum game, where:

	* Your winnings + your opponent's winnings = 0
	* ''Minimax theorems'': a rational strategy exists for all such finite games
	* Strategy: specification of which moves you make in which circumstances.
	* Equilibrium: each player's strategy is the best possible for their opponent's strategy.

This setup is simple because players don't have to think about cooperating with other players. In GAN, we are using a continuous game, but there is a special case where minimax does apply. When players take action randomly it is called ''mixed stategy'' and the equilibrium is called ''mixied strategy equilibrium''.

!! Adversarial nets game
The game is between two players: ''Discriminator'' $D$, and ''Generator'' $G$. The generator is the same thing as the decoder. The discriminater tries to discriminate between:

* A sample from the data distribution
* And a sample from the generator $G$

While $G$ tries to "trick" $D$ by generating samples that are hard for $D$ to distinguish from data. This is a game theory model, we try to do gradient descent on one side and ascent on the other.

The value function takes the form of a binary discrimination task:

$$
\underset{G}{\min}\underset{D}{\max} V(D, G) = E_{x\sim p_{data}}(x)[\log D(x)]+E_{z\sim p_z(x)}[\log(1-D(G(z)))]
$$

At every step, if the discriminator is optimal, it is going to take on this particular form, the optimal strategy for any $p_{model}(x)$ is always

$$
D(x) = \frac{p_{data}(x)}{p_{data}(x)+p_{model}(x)}
$$

!!! Noise contaction estimation
This is the same function used in noise contraction estimation. The different is that NCE is learning a model that used implicitly to define the discriminator, and here we're learning the generator. And in NCE, the generator is fixed. This function has a close connection to likelihood. 

MLE is NCE modified such that the discriminators believes the data distribution are copied into the generator network at every step. NCE is computationally cheaper because it does not update the generator. And because of this, it is failing to obtain the same asymptotic error rate as MLE.

!! Theoretical properties
This process has some nice theoretical properties, if we assume we can optimize directly over probability distributions, then there is one unique global optimum, this saddle point corresponds to the true distribution.  Assuming infinite data, infinite model capacity, direct updating of generator's distribution, converge to optimum guaranteed. But in practice, there is no proof that SGD will converge, where we are training gradient steps by opposite sign on the parameters of each model.

This is different from the usually analyzed statistical estimator, where we have an optimization problem where we minimize a function, and we show that in function space, the minimum of our objective function corresponds to recovering the true distribution. In this case we are not guaranteed to reach a critical point, that makes it more problematic when it comes to nonconvex optimization. In particular, we can have a network that oscilates forever.

! GANs vs VAEs
Both use backprop through continuous random number generation

* VAE:
** generator gets direct output target
** need Reinforce to do discrete latent variables
** possible underfitting due to variational approximation
** gets global image composition right but blurs details
* GAN
** generator never sees the data
** need Reinforce to do discrete visible variables
** possible underfitting due to non-convergence
** gets local image features right but not global structure

! Open problems

* Is non-convergence a serious problem in practice?
* If so, how can we prevent non-convergence?
* Is there a better loss function for generator?


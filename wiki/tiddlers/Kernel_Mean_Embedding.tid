created: 20160622120645080
modified: 20161103065330784
tags: [[Learning Theory]]
title: Kernel Mean Embedding
type: text/vnd.tiddlywiki

! Kernels
The kernel function arises as an effortless way to perform an inner product $\langle x, y\rangle$ in a high-dimensional feature space $\mathcal F$. The positive definiteness of the kernel function guarantees the existence of a dot product space $\mathcal F$ and a mapping $\phi:\mathcal X\rightarrow \mathcal F$ such that $k(x, y) = \langle \phi(x), \phi(y)\rangle_{\mathcal F}$. We will refer to $\phi$ and $k$ as a ''feature map'' and a ''kernel function'', respectively. Likewise, we can interpret $k(x, y)$ as a non-linear similarity measure between $x$ and $y$. 

For example, let's consider a polynomial feature map $\phi(x) = (x_1^2, x_2^2, x_1x_2, x_2x_1)$ when $x\in\mathbb R^2$. Then, we have
$$
\langle\phi(x), \phi(y)\rangle_{\mathcal F} = x_1^2y_1^2+x_2^2y_2^2+2x_1x_2y_1y_2 = \langle x, y\rangle^2
$$
the new similarity measure is simply the square of the inner product in $\mathcal X$. The equality holds more generally for a $d$-degree polynomial, i.e., $\phi$ maps $x\in\mathbb R^N$ to the vector $\phi(x)$ whose entries are all possible $d$th degree ordered products of the entries of $x$. We have $k(x, y) = \langle x, y\rangle^d$.

For algorithms depend only through the inner product of the data set, we can obtain a non-linear extension of these algorithms by substituting $\langle x, y\rangle$ with $k(x, y)$. 

Algorithms capable of operating with kernels include

* the kernel perceptron
* support vector machines (SVM)
* Gaussian processes
* principal components analysis (PCA)
* canonical correlation analysis
* ridge regression
* spectral clustering
* linear adaptive filters 

and many others. Any linear model can be turned into a non-linear model by applying the kernel trick to the model: replacing its features (predictors) by a kernel function. The capability of kernel trick also allows easy invention of domain-specific kernel functions. 

; p.s.d. kernel
: A function $k: \mathcal X\times\mathcal X\rightarrow \mathbb R$ is a positive semidefinite kernel if it is symmetric, i.e., $k(x, y) = k(y, x)$, and the Gram matrix is p.s.d.

A p.s.d. kernel defines a space of functions from $\mathcal X$ to $\mathbb R$ called a //reproducing kernel Hilbert space// (RKHS) $\mathscr H$.

We may view the kernel evaluation as an inner product in $\mathscr H$ induced by a map from $\mathcal X$ into $\mathscr H$: $x\rightarrow k(x,\cdot)$:
$$
k(x, y) = \langle k(x,\cdot),k(y,\cdot)\rangle_{\mathscr H}
$$

$k(x,\cdot)$ is a high-dimensional representer of $x$.

!! Mercer's theorem
Are wider range of psd kernels.

There is an intrinsic connection between the integral operator $\mathcal T_k$, covariance operator $\mathcal C_{XX}$, and Gram matrix $\mathbf K$. And there is a fundamental connection between Mercer's theorem in functional analysis and Karhunen-Loeve theorem of stochastic processes.

!! Bochner's theorem
It states that a function $f:\mathbf R^n\rightarrow \mathbf C$ is the Fourier tranform of a finite Borel measure iff $f$ is positive definite.
$$
\varphi(x-y) = \int_{\mathbb R^d}e^{\sqrt{-1}\omega^\top(x-y)}d\Lambda(\omega)
$$
The measure $\Lambda$ determines which frequency component occurs in the kernel by putting non-negative power on each frequency $\omega$. 

!! Schoenberg's theorem
Radial kernels: $k(x, y) = \varphi (\|x-y\|^2)$ where $\varphi$ is positive definite. Well known radial kernels include Gaussian RBF, mixture-of-Gaussians, inverse multiquadratic and Matern kernel. 

; Schoenberg's theoren
: Suppose $f: \mathbf R_+\rightarrow\mathbf R_+$ is continuous. Then, the following are equivalent:
# The function $\mathbf R^n\ni x \rightarrow f(\|x\|_n)$ is positive semi-definite.
# The function $\mathbf R^n\ni t \rightarrow f(\sqrt{t})$ is the Laplace transform of a finite Borel measure on $\mathbf R_+$.

Originally, this theorem was used to describe isometric embeddings of Hilbert spaces.

!! RKHS
A Hilbert space $\mathscr{H}$ of functions is a reproducing kernel Hilbert space if the evaluation functionals are bounded, i.e., $\forall x\in\mathcal X$ there exists some $C>0$ such that
$$
|f(x)|\le C\|f\|_{\mathscr{H}}
$$
This smoothness property ensures that the solution in RKHS obtained from learning algorithms will be well-behaved. For example, we obtain models $\hat f$ in classification and regression problems that is close to the true solution $f$ also generalize well to unseen test data, i.e., avoid overfitting.

A positive definite kernel defines a unique RKHS of functions from $\mathcal X$ to $\mathbb R$, so it is also called reproducing kernels. We define \textbf{canonical feature map} from $\mathcal X$ into $\mathbb R^{\mathcal X}:=\{f:\mathcal X\rightarrow\mathbb R\}$, via
\begin{align}
\phi:  \mathcal X&\rightarrow \mathscr H \subset \mathbb R^{\mathcal X}\\
x &\mapsto k(x,\cdot)
\end{align}

An inner product in $\mathscr H$ satisfies the \emph{reproducing property}, i.e., $\forall f\in\mathscr H$ and $x\in\mathcal X$:
$$
f(x) = \langle f, k(x,\cdot)\rangle_{\mathscr H}
$$
Take $k(y,\cdot)$ as $f$, we get $f(x) = k(y, x) = \langle\phi(x), \phi(y)\rangle_{\mathscr H}$. We do not need to know the feature map explicitly, the inner product can be calculated directly from $k(x, y)$.

! Hilbert Space Embedding of Marginal Distributions
The idea of Kernel mean embedding is to extend the feature map $\phi$ to the space of probability distributions by representing each distribution $\mathbb P$ as a mean function
$$
\phi(\mathbb P) = \mu_{\mathbb P} := \int_{\mathcal X}k(x, \cdot)d\mathbb P(x),
$$
where $k: \mathcal X\times\mathcal X\rightarrow \mathbb R$ is a symmetric and positive definite kernel function. We essentially transform the distribution $\mathbb P$ to an element in the feature space $\mathcal F$, which is nothing but a reproducing kernel Hilbert space endowed with the kernel $k$. Most KRHS methods can therefore be extended to probabiity measures. 

$$
\int f(t)d\delta_x(t) = \langle f, k(x, \cdot)\rangle_{\mathscr H}
$$


[[Kernel Trick Cheatsheet]]
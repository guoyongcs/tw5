created: 20161215074243368
modified: 20161226110955152
tags: [[Stochastic Gradient MCMC]]
title: Stochastic Gradient Langevin Dynamics
type: text/vnd.tiddlywiki

! Bibs
* [[Bayesian Learning via stochastic Gradient Langevin Dynamics|http://people.ee.duke.edu/~lcarin/398_icmlpaper.pdf]]
* [[A Variational Analysis of Stochastic Gradient Algorithms]]
** Langevin dynamics is the discrete-time approximation of a continous-time stochastic differential equation.
* [[Approximation analysis of stochastic gradient Langevin dynamics by using Fokker-Planck equation and Ito process|http://www.jmlr.org/proceedings/papers/v32/satoa14.html]]
** detailed convergence analysis
** [[video|http://techtalks.tv/talks/approximation-analysis-of-stochastic-gradient-langevin-dynamics-by-using-fokker-planck-equation-and-ito-process/61010/]]

This algorithm samples from a Bayesian posterior by adding artificial noise to the stochastic gradient which, at long times, dominates the SGD noise. Though elegant, one disadvantage of SGLD is that the learning rate must be decreased to achieve the correct sampling regime, and the algorithm can suffer from slow mixing times.

SG Fisher scoring speeds up mixing times in SGLD by preconditioning a gradient with the inverse sampling noise covariance.
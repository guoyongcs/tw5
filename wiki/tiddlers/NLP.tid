created: 20160425022224634
modified: 20170904084902803
tags: 
title: NLP
type: text/vnd.tiddlywiki

[[Stanford NLP Course]]

* [[NLP Data]]

! Topics
* Perceiving and representing text
** handling unknown words
** tokenize? spelling? characters? bytes?
** smaller perceptual units need more complex models, but have fewer OOVs (doubt)
* Text categorisation
* Natual language generation
** Language Modelling
*** [[DLSS 2017 Talks Phil Blunsom]]
** Conditional language modelling
*** speech recognition
*** caption generation
*** [[Question Answering]]
*** [[Dialog]]
*** [[Neural Machine Translation]]
*** following instructions
*** summarisation
* Analytic applications
** topic modelling
** linguistic analysis (discourse, semantics, syntax, morphology)
* [[CNN for NLP]]

! Tools
* [[LSTMVis|https://github.com/HendrikStrobelt/LSTMVis]]

! Task & State-of-the-art techniques

* Question answering (babl): Strongly Supervised MemNN (Weston et al. 2015) 93.3%; Dynamic Memory Network 93.6%
* Sentiment Analysis (SST): Tree-LSTMs (Tai et al. 2015); DMN 88.5%
* Part of speech tagging (PTB-WSJ): Bi-directional LSTM-CRP (Huang et al. 2015); 

The performance is always dominated by the size of high quality data.

<<<
Probably the most successful concept is to use distributed representation of words. For example, neural network based language models significantly outperform N-gram models.
<<< [[Efficient Estimation of Word Representations in Vector Space|Word2Vec]]

! Features
* BoW
* Distributed word embeddings
* Continuous ''vector'' representation of words
** co-occurrence stats to obtain vectors for long phrases
** LSA, fail to preserve linear regularities among words
** LDA, computationally expensive on large datasets
** [[Word2Vec]]
** [[GloVe|Global Vectors for Word Representation]]
* [[Hashed Character $n$-gram|https://arxiv.org/abs/1511.06018]]

! Sentence model
is to analysis and represent the semantic content of a sentence for purposes of classification or generation.

* sentiment analysis
* paraphrase detection
* entailment recognition
* summarisation
* discourse analysis
* grounded language learning
* image retrieval

!! Reviews
* Composition based methods for 
** vector representation of words or
** tied to particular syntactic relations or word types
* automatically extracted logical forms
* neural networks @@color:#d33682;
''advantages'': can be trained to obtain feature by predicting the context. can be powerful classifier/sentence generator
@@
** neural BoW or bag-of-//n//-grams
** recursive nets
** time-delay nets based on convolutional operations


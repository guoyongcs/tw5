created: 20150514071547700
modified: 20170908075159955
tags: [[Deep Neural Networks Architectures]] [[Fully-observed Models]]
title: Restricted Boltzmann Machines
type: text/vnd.tiddlywiki

RBMs can learn excellent generative models and RBM plays an important role in the training of Deep Belief Networks, as a good initialization for the FNN.
! Theory
!! Formalization
The RBM is a parameterized family of probability distributions over binary vectors. It defines a joint dsitribution over $v\in\{0, 1\}^{N_v}$ and $h\in\{0, 1\}^{N_h}$ via the following equation
$$
P(v, h) = \frac{\exp(h^\top Wv+v^\top b_v + h^\top b_h)}{Z(\theta)}
$$
The partition function $Z(\theta)$ is an sum of exponentially many terms and cannot be efficiently approximated to a constant multiplicative factor. The probability of the visible units is computed by marginalizing over the hidden units, also the probability of observing the data $X$, given the weights $W$:

$$p(X|W) = p(v) = \frac{1}{Z(v, h)}\sum_he^{-E(v, h)}$$

We can break the likelihood into 2 parts:
$$
\mathcal L = \ln p(v) = \ln\sum_he^{-E(v, h)}-\ln Z(v, h)
$$
//Clamped// ''Free Energy'' $F^c(v)$ and the standard ''Free Energy'' $F(v, h)$. First one is easy to evaluate in the RBM formalism, whereas $F(v, h)$ is computationally intractable. Knowing the $Z(v, h)$ is //like// knowing the equilibrium distribution function, and methods like RBMs appear to approximate it in some form or another.

!! Training
RBMs are usually trained via Contastive Divergence. The Energy funtion, being quadratic, lets us readily factor $Z$ using a mean field approximation, leading to simple expressions for the conditional probabilities:
$$
\begin{align}
p(h_i=1|v) &= \sigma(b_i+W_ih)\\
p(v_i=1|h) &= \sigma(a_i+W_iv)
\end{align}
$$
and weight update rule
$$
dw = \langle v, h\rangle_+-\langle v, h\rangle_\infty
$$

$\langle v, h\rangle_\infty$ is evaluated in the limit of infinite sampling, at the so-called equilibrium distribution. But we don't take the infinite limit.

CD approximates the (mean field) Free Energy by running only 1 (or more) steps of Gibbs Sampling.

! Applications
RBMs are basicly a solved problem. 10 years ago, we use them to pretrain deep supervised nets. They still outperform VAEs on simple datasets. RBM research continues in areas like semi-supervised deep learning with deep hybrid architectures, temperature dependence, infinitely deep RBMs, etc.

! Partition Functions related with Quantum Chemistry

refs

* [[Improving RBMs with physical chemistry|https://charlesmartin14.wordpress.com/2016/10/21/improving-rbms-with-physical-chemistry/]]

<<<
Knowing the Parittion function $Z(v, h)$ is not the same as knowing the distribution $p(v)$.
<<< [[Why Does Deep and Cheap Learning Work so Well?]]

This is a small but important technical oversight. Scaling Energy does not change $Z(v, h)$. A connection between RBMs and Kadanoff VRG approach is explained in [[Charlse's post|https://charlesmartin14.wordpress.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/]]. A more detailed connection between deep learning and Statistical Mechanics is shown [[here|An exact mapping between the Variational Renormalization Group and Deep Learning]]. 

<<<
RG procedure must preserve more information about the distribution than the free energy.
<<< [[Comment on “Why does deep and cheap learning work so well?”|http://arxiv.org/pdf/1609.03541v1.pdf]]

Also in the training of RBMs, the idea of preserving Free Energy, via either a ''trace condition'' or say, by layer normalization, is the important point. In variational RG, on the renomalization operator $T(v, h)$:
$$
\text{Tr}_he^{T(v, h)}=1
$$

similar to ''Size-Consistency and/or Size-Extensivity'' in Quantum Chemistry.

! Bibs
* [[Recurrent Temporal RBM]]
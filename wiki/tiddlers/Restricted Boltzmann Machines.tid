created: 20150514071547700
modified: 20160929061032710
tags: [[Deep Learning]]
title: Restricted Boltzmann Machines
type: text/vnd.tiddlywiki

RBMs can learn excellent generative models and RBM plays an important role in the training of Deep Belief Networks, as a good initialization for the FNN.

! Formalization
The RBM is a parameterized family of probability distributions over binary vectors. It defines a joint dsitribution over $v\in\{0, 1\}^{N_v}$ and $h\in\{0, 1\}^{N_h}$ via the following equation
$$
P(v, h) = \frac{\exp(h^\top Wv+v^\top b_v + h^\top b_h)}{Z(\theta)}
$$
The partition function $Z(\theta)$ is an sum of exponentially many terms and cannot be efficiently approximated to a constant multiplicative factor. The probability of the visible units is computed by marginalizing over the hidden units, also the probability of observing the data $X$, given the weights $W$:

$$p(X|W) = p(v) = \frac{1}{Z(v, h)}\sum_he^{-E(v, h)}$$

We can break the likelihood into 2 parts:
$$
\mathcal L = \ln p(v) = \ln\sum_he^{-E(v, h)}-\ln Z(v, h)
$$
//Clamped// ''Free Energy'' $F^c(v)$ and the standard ''Free Energy'' $F(v, h)$. First one is easy to evaluate in the RBM formalism, whereas $F(v, h)$ is computationally intractable. Knowing the $Z(v, h)$ is //like// knowing the equilibrium distribution function, and methods like RBMs appear to approximate it in some form or another.

! Training
RBMs are usually trained via Contastive Divergence. The Energy funtion, being quadratic, lets us readily factor $Z$ using a mean field approximation, leading to simple expressions for the conditional probabilities:
$$
\begin{align}
p(h_i=1|v) &= \sigma(b_i+W_ih)\\
p(v_i=1|h) &= \sigma(a_i+W_iv)
\end{align}
$$
and weight update rule
$$
dw = \langle v, h\rangle_+-\langle v, h\rangle_\infty
$$

$\langle v, h\rangle_\infty$ is evaluated in the limit of infinite sampling, at the so-called equilibrium distribution. But we don't take the infinite limit.

CD approximates the (mean field) Free Energy by running only 1 (or more) steps of Gibbs Sampling.
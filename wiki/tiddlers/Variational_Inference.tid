created: 20150114062559893
modified: 20161208114826019
tags: [[Pattern Recognition and Machine Learning]] [[Probabilistic Inference]]
title: Variational Inference
type: text/vnd.tiddlywiki

$$
\ln q_j^*(\mathbf Z_j) = \mathbb E_{i\ne j}[\ln p(\mathbf X, \mathbf Z)]+C
$$

Following borrowed from [[here|http://www.inference.vc/variational-renyi-lower-bound/]].
! Why do we need variational lower bounds?

One way to define a probabilistic generative model for some variable $x$ is via latent variables: We introduce hidden variables $z$ and define the joint distribution between $x$ and $z$. In such a model, typically:

* $p(z)$ is very easy 🐣,
* $p(x|z)$ is easy 🐹,
* $p(x,z)$ is easy 🐨,
* $p(x)$ is super-hard 🦂,
* $p(z|x)$ is mega-hard 🐉
to evaluate.

Unfortunately, in machine learning the things we need to calculate are exactly the bad guys, 🦂 and 🐉:

* inference is evaluating $p(z|x)$
* learning (via maximum likelihood) involves $p(x)$ or at least its gradients

Variational lower bounds give us ways to approximately perform both inference and maximum likelihood parameter learning.

! Standard variational (VI) lower bound
To nice auxiliary distribution $q(x,\psi)$ 🦄, that is both easy to evaluate analytically and easy to sample from, and define the lower bound as follows:
$$
L(x,\theta,\psi)=\log p(x;\theta)−\operatorname{KL}[q(z;\psi)\|p(z|x;\theta)],
$$
where $\theta$ are the parameters of the generative model. Hey, but doesn't that formula have the two things we can't evaluate? The good thing is that the lower bound can be rearranged to a form where we don't need either of those:
$$
L(x,\theta,\psi)=\mathbb E_q\log p(x,y;\theta)q(x;\psi)
$$
So basically:
$$
\log(🦂)−\operatorname{KL}[🦄\|🐉]=\mathbb E_🦄\log\frac{🐨}{🦄}
$$
so we only have to deal with koalas and unicorns. Sweet.

For the drawbacks of VI and ''Rényi Lower Bound'', refer to [[Rényi Divergence Variational Inference|https://arxiv.org/abs/1602.02311]] and [[this blog|http://www.inference.vc/variational-renyi-lower-bound/]].

! Relationship with EM
$\mathbb E_🐉\log🐨$ is the ''expected complete log-likelihood'', which is optimized by EM. Variational inference does not estimate fixed model parameters, it is often used in a Bayesian setting where classical parameters are treated as latent variables. Variational inference applies to models where we cannot compute the exact conditional of the latent variables.

! Coordinate ascent mean-field variational inference
CAVI fixes other variational factors and compute the optimal $q_j(z_j)$ by its ''complete conditional''. This can be seen as a ''message passing'' algorithm, and is closely related to Gibbs sampling.
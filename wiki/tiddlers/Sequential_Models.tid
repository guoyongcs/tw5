created: 20161012083717362
modified: 20170328110016967
tags: [[Recurrent Neural Networks]]
title: Sequential Models
type: text/vnd.tiddlywiki

! Models

* [[LSTM|Long short-term memory]]
* Tapes
* Arrays
* Stacks
* [[Quasi-RNN]]

!! Bibs

* [[Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets]]
* [[Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision|https://arxiv.org/abs/1611.00020]]
** non-differentiable memory optimized with REINFORCE, improved weakly supervised semantic parsing on WebQuestionsSP.
* [[Differentiable neural computers|https://deepmind.com/blog/differentiable-neural-computers/]]
** DeepMind's nature paper
* [[Tracking the World State with Recurrent Entity Networks|https://openreview.net/forum?id=rJTKKKqeg]]
** [[tensorflow implementation|https://github.com/jimfleming/recurrent-entity-networks]]
** the results aren't easily comparable to DNC

! Memory networks
Memory-augmented networks

* Memory Network
* End-To-End Memory Network: make the supervision indirect, supervision signal on output
* Key-Value Memory Network: makes matchings more relavant
* Forward Prediction Memory Network
* Recurrent Entity Network

Some related models

* NTM
* Stack RNNs
* Dynamic Mem. Nets
* DNC

!! Bibs

* [[MemN2N|http://arxiv.org/abs/1604.06045]]
* [[Can Active Memory Replace Attention?|https://arxiv.org/abs/1610.08613]]
** Extended Neural GPU with an output tape tensor $p$, which has access to @@color:#859900;all previous outputs@@.
** Use teacher forcing in training.
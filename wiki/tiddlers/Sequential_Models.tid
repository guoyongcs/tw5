created: 20161012083717362
modified: 20170503094731806
tags: [[Recurrent Neural Networks]]
title: Sequential Models
type: text/vnd.tiddlywiki

! Models

* [[LSTM|Long short-term memory]]
* Tapes
* Arrays
* Stacks
* [[Quasi-RNN]]

!! Memory networks
* [[Memory Networks]]
* [[Neural Machine Translation by Jointly Learning to Align and Translate|https://arxiv.org/abs/1409.0473]]
* (Stack) RNNs with attention
* Dynamic Mem. Nets
* [[Neural Abstract Machines]]

! Bibs

* [[Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets]]
* [[Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision|https://arxiv.org/abs/1611.00020]]
** non-differentiable memory optimized with REINFORCE, improved weakly supervised semantic parsing on WebQuestionsSP.
* [[Differentiable neural computers|https://deepmind.com/blog/differentiable-neural-computers/]]
** DeepMind's nature paper
* [[Tracking the World State with Recurrent Entity Networks|https://openreview.net/forum?id=rJTKKKqeg]]
** [[tensorflow implementation|https://github.com/jimfleming/recurrent-entity-networks]]
** the results aren't easily comparable to DNC



Most networks with external memory use some form of ''content-based'' memory access, find the memory closest to some ''key vector'' emitted by the network, return either the memory content or an associated ''value vector''.

!! Bibs

* [[MemN2N|http://arxiv.org/abs/1604.06045]]
* [[Can Active Memory Replace Attention?|https://arxiv.org/abs/1610.08613]]
** Extended Neural GPU with an output tape tensor $p$, which has access to @@color:#859900;all previous outputs@@.
** Use teacher forcing in training.
created: 20161103092937981
modified: 20170917102130440
tags: [[ConvNet Layers]]
title: Normalization Layer
type: text/vnd.tiddlywiki

! Bibs

* [[L2 Regularization versus Batch and Weight Normalization|https://arxiv.org/abs/1706.05350]]
** $L_2$ regularization, in combination with these normalization layers, has no regularization effect, rather strongly influences the learning rate
** But learning rate in return is a regularization

! Introduction

$$
y_{NN}(X;\mathbf w, b) = g(X\mathbf w + b)
$$

* [[Batch Normalization]]: $y_{BN}(X;\mathbf w, \gamma, \beta) = g(\frac{X\mathbf w - \mu(X\mathbf w)}{\sigma(X\mathbf w)}\gamma + \beta)$
* [[Weight Normalization]]: $y_{WN}(X;\mathbf w, \gamma, \beta) = g(\frac{X\mathbf w}{\|\mathbf w\|_2}\gamma + \beta)$
* [[Layer Normalization|https://arxiv.org/abs/1607.06450]]: $y_{BN}(\mathbf x;W, \gamma, \beta) = g(\frac{\mathbf xW - \mu(\mathbf xW)}{\sigma(\mathbf xW)}\gamma + \beta)$
**  instead of taking the statistics of a single unit over a whole batch of inputs, they are taken for a single input over all units in a layer
created: 20161019092508242
modified: 20170602080516956
tags: [[Deep Learning Applications]]
title: Deep Generative Models
type: text/vnd.tiddlywiki

! Overview
* [[Deep Generative Model Overview]]

! Models

* Denoising Autoencoder
** [[Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space|https://arxiv.org/abs/1612.00005]]
* [[Variational Autoencoder]]
* [[Generative Adversarial Network]]
* [[Input Convex Neural Network]]
* [[Variational Generative Models]]
* [[TCL|Time-Contrastive Learning]]
* Inverse Autoregressive Flow
** [[paper|https://arxiv.org/abs/1606.04934]], [[tf implementation|https://github.com/openai/iaf]]
* [[Real NVP|http://arxiv.org/abs/1605.08803]]
* [[Attentive Generative Models]]

!! Autoregressive models

* [[WaveNet]]
* [[Pixel-CNN]]
* [[SampleRNN]]
* Video Pixel Network
* ByteNet
* [[A Neural Parametric Singing Synthesizer|http://arxiv.org/abs/1704.03809v1]]
** What is the Causual conv input?
** How's the model trained
** How's the time sequence represented
** Hints for OCR context?

Autoregressive models are trained and evaluated by teacher-forcing, meaning the model receives as input the true values of the variables being conditioned on. During generation, however, these variables are not known, and the model conditions on its own best predictions of their values. This difference between training/evaluation and generation is a known source of trouble: the model only ever learns one-step transitions, and never learns to deal with its own errors. At generation time it can easily run off the tracks.

[[GANs vs VAEs]]

! Bibs

* [[yingzhen's reading list|http://www.yingzhenli.net/home/blog/?p=566]]
* [[Magenta's reviews|https://github.com/tensorflow/magenta/tree/master/magenta/reviews]]
* [[What does it take to generate natural textures?]]

on evaluation

* [[On the Quantitative Analysis of Decoder-Based Generative Models|https://openreview.net/forum?id=B1M8JF9xx&noteId=B1M8JF9xx]]

! Applications
* [[Neural Style]]
* [[High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis|https://arxiv.org/abs/1611.09969]]
** Autoencoder with perceptual loss
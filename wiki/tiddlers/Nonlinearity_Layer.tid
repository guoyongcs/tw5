created: 20161103090428046
modified: 20161103090622574
tags: [[ConvNet Layers]]
title: Nonlinearity Layer
type: text/vnd.tiddlywiki

! ReLU
ReLU is the abbreviation of Rectified Linear Units. This is a layer of neurons that use the non-saturating activation function $f(x) = \max(0, x)$. It increases the nonlinear properties of the decision function and of the overall network without affecting the receptive fields of the convolution layer.

Other functions are used to increase nonlinearity. For example the saturating hyperbolic tangent $f(x) = \tanh(x)$, $f(x) = |\tanh(x)|$, and the sigmoid function $f(x) = (1 + e ^{-x})^{-1}$. Compared to tanh units, the advantage of ReLU is that the neural network trains several times faster.

''advantages'': 

# enforces hard zeros in representation.
# avoids vanishing gradient somehow.

! Variations

* [[ELU|https://arxiv.org/abs/1511.07289]]: openai uses this on ResNet VAEs for IAF.
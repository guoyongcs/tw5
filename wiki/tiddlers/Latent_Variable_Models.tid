created: 20170908080220935
modified: 20170908083523724
tags: [[Probabilistic Models]]
title: Latent Variable Models
type: text/vnd.tiddlywiki

! Model Zoo

|>|!Direct/Linear|<|
|! |!Parametric |!Non-parametric |
|!Discrete |<li>[[Hidden Markov Model]]</li><li>Discrete LVM</li><li>Sparse LVM</li> |<li>Indian Buffet Process</li><li>[[Dirichlet Process Mixtures]]</li> |
|!Continuous |<li>PCA</li><li>[[Factor Analysis]]</li><li>Independent Component Analysis</li><li>Gaussian LDS</li><li>Latent Gauss Field</li> |<li>Gaussian Process LVM</li> |
|>|!Deep|<|
|! |!Parametric |!Non-parametric |
|!Discrete |<li>Sigmoid Belief Net</li><li>Deep Auto-Regressive Networks</li> |<li>Cascaded Indian Buffet Process</li><li>[[Hierarchical Dirichlet Process|HDP-HMM Diarization]]</li> |
|!Continuous |<li>Nonlinear Factor Analysis</li><li>Nonlinear Gaussian Belief Network</li><li>Deep Latent Gaussian ([[VAE|Variational Autoencoder]], [[DRAW|Variational Inference for Generative Models]])</li> |<li>Deep Gaussian Process</li><li>Recurrent Gaussian Process</li><li>GP State Space Model</li> |

!! Conjugate
* Bayesian mixture models
* Times series models (HMM, linear dynamic systemsï¼‰
* Factorial models
* Matrix factorization (factor analysis, PCA, CCA)
* [[Dirichlet Process Mixtures]], HDPs
* Multilevel regression (linear, probit, Poisson)
* Stochastic block models
* Mixed-membership models (LDA and some variants)

!! Nonconjugate
* Nonlinear Time series models
* Deep latent Gaussian models
* Models with attention (DRAW)
* Generalized linear models (Poisson regression)
* Stochastic volatility models
* Discrete choice models
* Bayesian neural networks
* Deep exponential families (sparse Gamma or Poison)
* Correlated topic models (nonparametric variants)
* Sigmoid belief network

! Remarks

@@color:#859900;
* Easy sampling
* Easy way to include hierarchy and depth
* Easy to encode structure
* Avoids order dependency assumptions: marginalisation induces dependencies.
* Provide compression and representation
* Scoring, model comparison and selection possible using the marginalised likelihood
@@

@@color:red;
* Inversion process to determin latents corresponding to an input is difficult to generalize
* Difficult to compute marginalised likelihood requiring approximations
* Not easy to specify rich approximations for latent posterior distribution
@@

! Learning Principles



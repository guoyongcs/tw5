created: 20150514055814821
modified: 20161111103600780
tags: [[Deep Neural Networks Architectures]]
title: Recurrent Neural Networks
type: text/vnd.tiddlywiki

! Introduction

RNN is a very deep feedforward neural newwork that has a layer for each timestep. Its weights are shared across time.

The gradients of the RNN are easy to compute with BPTT, RNNs are difficult to train, especially on problems with long-range temporal dependencies due to their nonlinear iterative nature. The derivative of the loss function at one time can be exponentially large w.r.t. the hidden activations at a much earlier time.

!! Vanishing gradient problem
Here is a second order approach: [[Thesis Training RNN]]. Not considered a good solution now.

! [[Sequential Models]]

! Bibs
* [[
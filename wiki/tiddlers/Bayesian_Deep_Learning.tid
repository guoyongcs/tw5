created: 20170621062553683
modified: 20170719080022094
tags: Bayesian [[Deep Learning]]
title: Bayesian Deep Learning
type: text/vnd.tiddlywiki

! History
First Bayesian NNs are training with Laplace's method. Neal uses HMC. In late 90s, VI is used.

Modern VI relied on a fully factorized approximation, assuming independence of each weight scalar in each layer from all other weights.

Expectation propagation is recently studied, relying on various forms of Renyi's $\alpha$-divergence. Seems to sacrifice their estimation of the dominant modes of the posterior.

! Introduction

Bayesian deep learning is

<<<
Posterior inference of layered representations of high-dimensional data
<<< Ranganath+, AISTATS 2015

Topics

* Deep exponential families: $z_{n,l,k}\sim \text{Exp-Fam}(g(w^\top_{l,k}z_{n,l+1}))$
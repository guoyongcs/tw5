created: 20161008091826135
modified: 20170831032436057
tags: [[Deep Learning Applications]]
title: Reinforcement Learning
type: text/vnd.tiddlywiki

! Tutorials

* [[WildML: Learning Reinforcement Learning|http://www.wildml.com/2016/10/learning-reinforcement-learning/]]
* [[Simple Reinforcement Learning with Tensorflow|https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0]]

! Basic
!! RL agents
* value based, has a value function where policy can be inferred.
* policy based, work in decision space, more direct. Policy Graident.
* [[Actor-Critic Method]]: stores policy and value
* model free/based

!! Anatomy of a RL algorithm
Iterate these 3 stepsï¼š

# Generate samples (i.e. run the policy)
# Fit a model to estimate return
#* MC policy gradient: compute $\hat Q = \sum_{t'=t}^T\gamma^{t'-t}r_{t'}$
#* [[Actor-Critic Method]], [[Q-Learning]]: fit $Q_\phi(s, a)$
#* Model-based: estimate $p(s'|s, a)$
# Improve the policy
#* [[Policy Gradient]]: $\theta\leftarrow\theta+\alpha\nabla_\theta J(\theta)$
#* Q-learning: $\pi(s) = \arg\max Q_\phi(s, a)$
#* Model-based: optimize $\pi_\theta(a|s)$

!! Tricks

* Freezing target network
* Error clipping, reward clipping
* Experience replay

!! Training steps
On training half-cheetah, each one is 10x easier than previous. (each episodes is 1000 steps)

* gradient-free methods, e.g. evolution strategies, CMA, etc. 
* fully online methods, e.g. A3C, 100,000 episodes ~15 days
* policy gradient methods, e.g. TRPO, 10,000 episodes
* relay buffer value estimation, e.g. Q-learning, DDPG, NAF, etc., 1,000 episodes
* model-based deep RL, e.g. guided policy search
* model-based shallow RL, e.g. PILCO

! Bibs
* [[Faster Deep Reinforcement Learning]]
* [[Playing Doom with SLAM-Augmented Deep Reinforcement Learning|https://arxiv.org/abs/1612.00380]]
* [[Learning to Communicate|https://blog.openai.com/learning-to-communicate/]]: differentiable language for RL
* [[Curiosity-driven Exploration by Self-supervised Prediction|https://pathak22.github.io/noreward-rl/]]: training with sparse reward

! Topics
* [[Imitation Learning]]
* [[Meta-Learning]]
* [[Noisy Networks for Exploration|https://arxiv.org/abs/1706.10295]]
* [[Soft Optimality]]

! Implementations
Environments

* [[CommAI-env|https://github.com/facebookresearch/CommAI-env]]
* [[Universe|https://openai.com/blog/universe/]] with [[Gym|https://gym.openai.com/]]
* [[DeepMind Lab|https://github.com/deepmind/lab]]
* [[Facebook ELF|https://github.com/facebookresearch/ELF]]
created: 20161116081814547
modified: 20170905030931739
tags: [[Recurrent Neural Networks]] Optimization
title: Optimizing RNN
type: text/vnd.tiddlywiki

The gradients of the RNN are easy to compute with BPTT, RNNs are difficult to train, especially on problems with long-range temporal dependencies due to their nonlinear iterative nature. The derivative of the loss function at one time can be exponentially large w.r.t. the hidden activations at a much earlier time.

! Vanishing gradient problem
Here is a second order approach: [[Thesis Training RNN]]. Not considered a good solution now.

Merity has [[a blog post|https://smerity.com/articles/2016/orthogonal_init.html]] about using orthogonal initialization to avoid explosion and vanishing in  propagation, and has referred to some optimization papers.

! [[Optimization Properties of Linearized RNN]]
created: 20170901072750976
modified: 20170905024607844
tags: NLP Talks
title: DLSS 2017 Talks Phil Blunsom
type: text/vnd.tiddlywiki

[[Part I|http://videolectures.net/deeplearning2017_blunsom_language_understanding/]]

Much of NLP (e.g. Translation, QA, Dialogue) can be structured as (conditional) language modelling. The simple objective of modelling the next word contains much of the complexity of nature language understanding

<<<
@@color:red;Alice@@ went to the @@color:blue;beach@@. @@color:blue;There@@ @@color:red;she@@ build a ...
<<<

Models can be evaluated with cross entropy, a measure of how many bits are needed to encode text with our modelï¼š
$$
H(w^N_1) = -\frac1N\log_2p(w^N_1)
$$
or perplexity, a measure of how surprised our model is on seeing each word:
$$
\text{perplexity}(w_1^N) = 2^{H(w_1^N)}
$$

* Count based N-Gram Models
** Back-off to lower interpolation: if never observe the trigrams, try bigrams
** Heap's Law: smoothing techniques match the empirical distribution of languge
** hard to capture long dependencies, semantic correlations and morphological regualrities.
* Neural N-Gram Language Models
* Recurrent Neural Network Language Models
* Encoder-Decoder Models and Machine Translation

Truncated BPTT is efficient for mini-batching as all sequences have length $T$.
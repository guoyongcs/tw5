created: 20160822072835753
modified: 20161027082935324
tags: [[Deep Learning Tricks]]
title: Neural Nets Optimizations
type: text/vnd.tiddlywiki

! Tricks
# [[Batch Normalization]]: addressing the problem of the internal covariate shift
# learning with a curriculum (Bengio et al., 2009)
# training with diffusion (Mobahi, 2016) - a form of continuation method
# noisy activation functions improve performance on a wide variety of tasks (Gulcehre et al., 2016)
# [[Mollifying Networks]]: 

Impact of noise injection (Neelakantan et al., 2015)

!! Using mini-batches
Using mini-batchesof examples, as opposedto one example at a time, is helpful in several ways. First, the gradient of the loss over a mini-batch is an estimate of the gradient over the training set, whose quality improves as the batch size increases. Second, computation over a batch can be much more efficient than m computations for individual examples, due to the parallelism afforded by the modern computing platforms.